#!/usr/bin/python

import time
from src.core import TweetDragger
import json

if __name__ == '__main__':
	import argparse

	parser = argparse.ArgumentParser(description='Twitter account scraper and tweet displayer')
	parser.add_argument('account', type=str, help='twitter handle to scrape')
	parser.add_argument('--dump', action='store_true', help='print out a json dump of all collected tweets for the specified user')
	parser.add_argument('--timer', type=int, default=10, help='How many minutes to wait beween scrapes')
	args = parser.parse_args()

	username = args.account
	dumpjson = args.dump
	if args.timer <= 0:
		args.timer = 10
	sleepDuration = args.timer * 60

	if dumpjson:
		try:
			with open('{}.json'.format(username), 'r') as fp:
				jsonData = json.load(fp)
				print(json.dumps(jsonData, indent=4))
		except FileNotFoundError:
			print("No tweets stored for {}".format(username))
		quit()

	dragger = TweetDragger(username)
	dragger.populate()

	dragger.printRecentTweets(5) # print 5 most recent tweets
	print()

	while True:
		with open('{}.json'.format(username), 'w') as fp:
			json.dump(dragger.tweets, fp, indent=4)

		print('\tChecking for new tweets in {} minute(s)\n'.format(int(sleepDuration/60)))

		# store the most recent tweet so we know where to pick up from
		latestTweet = dragger.getLatestTweet()

		time.sleep(int(sleepDuration))

		dragger.update()

		dragger.printNewTweetsSince(latestTweet)